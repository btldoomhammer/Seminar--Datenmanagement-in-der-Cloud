\chapter{Zusammenfassung und Ausblick}
\label{sec:Zusammenfassung-Ausblick}

\section{Zusammenfassung}
\label{sec:Zusammenfassung}

\section{Andere Ansätze}
\label{sec:AndereAnsatze}
Die in dieser Ausarbeitung im Detail vorgestellten Algorithmen sind nur ein Teil der Möglichkeiten, mehrere Prozesse für Datenbankabfrage-Optimierung zu nutzen. Um einen kleinen Blick auf diese anderen Ansätze zu werfen, werden im folgenden einige der Interessanteren betrachtet. Dazu gehört der Einsatz spezieller Hardware wie beispielsweise einen Netzwerkprozessor, einem Prozessor mit spezieller Prozess-optimierter Architektur. Aber auch die GPU Programmierung mit CUDA kann genutzt werden, um das Potenzial bereits Vorhandener Hardware miteinzubeziehen. Eine ganz andere Strategie verfolgt der Ansatz der multiplen Datenbankengines. Hier wird versucht auf die Entwicklung aufwendiger neuer Algorithmen zu verzichten indem ein einzelner Rechner wie ein Verteiltes System betrachtet wird und jeder Prozessorkern seine eigene Datenbankengine verwaltet.

Diese drei Ansätze werden im folgenden kurz angerissen, es gibt jedoch weitere Ansätze und Algorithmen, die hier nicht mehr erwähnt werden. Die meisten Dieser ähneln jedoch denen im Detail vorgestellten Algorithmen indem sich versuchen, die Zugriffe auf den Speicher zu optimieren und Cache-Misses und Pipeline-Abbrüche zu reduzieren.

\subsection{Netzwerkprozessor}
\label{sec:Netzwerkprozessor}

Ein Ansatz mehrere Prozesse optimaler zu unterstützen ist es, spezielle Prozessoren zu verwenden, die besser geeignet sind mit einer großen Anzahl von Prozessen umzugehen, jedoch auf die normalen Schwierigkeiten verzichten, die bei einem normalen Prozessor anfallen. (Quelle) Automatisierte Vorgänge wir Prefetching und ein großer Overhead beim Wechsel von einem Prozess in den nächsten erzeugen Grenzen, wie viele Prozesse effektiv verwendet werden können. Alternativ gibt es Netzwerkprozessor. Diese sind spezielle Prozessoren die sich durch eine auf Prozesse spezialisierte Architektur auszeichnen. Pro Prozessorkern – Microengine genannt – können bis zu 8 Prozesse laufen und durch spezielle Register und Cache kann ein Kontextwechsel in einem einzelnen Prozessorzyklus durchgeführt werden, was den Overhead drastisch reduziert. Auch die die Latenzzeit für den Zugriff auf den internen Speicher drastisch geringer.

Durch die spezielle Art der Hardware wird jedoch der Programmieraufwand größer. Viele automatisierte Vorgänge müssen durch das Programm selber durchgeführt werden. Dafür kann aber auch genau gesagt werden, welcher Prozess wann auf dem Kern arbeitet. Ein Prozesswechsel muss explizit angegeben werden und der nächste Prozess wird nach dem Round-Robin Verfahren ausgewählt. Zusätzlich gibt es nur einen einfachen Hardware-Prefetcher. Dadurch kann der Programmierer selber festlegen, welche Daten permanent im Cache gehalten werden sollen. Dies ist gerade bei Programmen mit komplexem Programm-Fluss eine zusätzliche Anforderung an die Programmierung, aber es wird dadurch gleichzeitig gewährleistet, dass kritische Daten auch wirklich immer im Speicher liegen und nicht erst nach einem Cache-Miss durch den Hardware-Prefetcher neu geladen werden müssen. Gleichzeitig können Daten, von denen bekannt ist, dass sie für den Algorithmus uninteressant sind, im RAM belassen werden und müssen nicht in den Cache passen.

Ein Sequenzieller Scan einer Tabelle einer Datenbank kann beispielsweise so aufgeteilt werden, dass eine Page pro Microengine durchsucht wird. Die 8 Prozesse der Microengine können dann parallel jeweils einen Eintrag laden und bearbeiten. Und da die Kontrolle über den Cache besser kontrollierbar ist, können alle relevanten Daten der Page in den Cache der Microengine geladen werden und müssen nicht erst durch Cache-Misses, also Pipeline-Abriss, neu geladen werden.

Beim HashJoin haben wir es jedoch mit einem anderen Problem zu tun, da der Netzwerk-Prozessor  (oder zumindest der in der Quelle verwendete) nur über wenig zusammenhängendem Cache verfügt, und so die Gefahr besteht, dass die Buckets der Hashtabelle bei der Erzeugung zu groß werden können. Da ihre Größe nicht effektiv vorhergesehen werden kann muss die Hashtabelle sicherheitshalber im RAM belassen werden. Es entsteht ein größerer Overhead beim Zugriff auf die Tabellen. Aber da wird die einzelnen Pages wie beim sequenziellen Scan an verschiedene Micoroengines aufteilen können, haben wir dennoch durch die große Anzahl von Parallelität auf Prozessebene einen zeitlichen Gewinn.

Die Verwendung eines Netzwerkprozessors ist aufgrund des umfangreicheren Programmieraufwandes nicht einfach, kann aber sicherlich mit anderen Optimierungen und Algorithmen verwendet werden. Vermutlich lassen sich die in dieser Ausarbeitung vorgestellten Algorithmen und Ansätze auch auf einen Netzwerkprozessor übertragen, in manchen Fällen vielleicht sogar effektiver als auf einem herkömmlichen Prozessor. Für große Datenbanken ist es sicherlich Vorteilhaft, einen Netzwerkprozessor einzusetzen.


\subsection{CUDA}
\label{sec:CUDA}

Im Vergleich zu CPUs besitzen die meisten modernen GPUs (Grafikprozessoren) eine ca 10x höhere Rechenleistung und 10x größere Speicherbandbreite. Zusätzlich sind sie bereits für Multiprozess-Tätigkeiten optimiert, bieten schnelle Kommunikation zwischen Prozessen und erlauben zufälligen Zugriff auf Speicheradressen. Dies wird durch on-chip Speicher erreicht, der zusätzlich zum schnellen Grafikspeicher auf der Grafikarte selber id der GPU verwendet wird. All dies sind Eigenschaften, die bei einer Multiprozess-Datenbankanfrage erforderlich sind um diese optimal zu unterstützen. Warum es bis jetzt jedoch noch nicht zum Einsatz kommt, liegt an einem fundamentalen Problem der Rechnerarchitektur. Die Kommunikation zwischen CPU und GPU geschiet über den BUS, der jedoch nur begrenzte Bandbreite besitzt. Dadurch kann die GPU nicht effektiv auf den RAM des Rechners zugreifen. Diese Grenze zwischen GPU und CPU existiert in beide Richtungen und erschwert den Einsatz der GPU. Das heißt jedoch nicht, dass die Verwendung der GPU unmöglich ist. All die Operationen, die auf einer festen Datenmenge basieren können auf der GPU durchgeführt werden. Die Daten müssen einmalig auf die GPU übertragen werden und können dort gehalten werden.

Über die Programmiersprache CUDA kann die GPU programmiert werden und das nicht nur für grafische Anwendungen. Kenntnis der Grafikpipelines sind nicht erforderlich. Mit der Hilfe von Cuda kann die GPU in einen Coprozessor verwandelt werden, der Relationsanfragen bearbeiten kann und dadurch die Arbeitskraft der CPU für andere Speicherzugriffs-lastige Aufgaben freimacht. (Quelle: Relational Query Coprocessing on Graphics Processors) Optimierte Algorithmen sind jedoch erforderlich, um den Nachteil der Bustransferrate auszugleichen. Dazu müssen die Features der GPU voll ausgenutzt werden. Die massive Prozess-Parallelität, die schnelle Inter-Prozess-Kommunikation und der gemeinsam genutzte Speicher sind hierbei die Kernfeatures. Pro Prozessorkern auf der Grafikkarte laufen ganze Prozessgruppen. SIMD Unterstützung und Architektur sind einem Netzwerkprozessor nicht unähnlich. Durch SIMD können mehrere Prozesse die selbe Arbeit verrichten, nur dass sie andere Daten verarbeiten. Das bedeutet aber zusätzlich, dass unterschiedliche Befehle nicht parallel ausgeführt werden können. Dies muss zusätzlich in dem Ansatz berücksichtigt werden. Dafür setzt das in [Quelle] erstellte System einen Abschätzungsmechanismus ein, um Aufgaben, die Idealerweise auf der GPU verarbeitet werden könne, an diese weiterzugeben. Hierbei wird die Bus-Transferrate mit berücksichtigt um einen Leistungsgewinn zu erzielen. Durch die Bus-Limitierung ist es erforderlich, die kommunizierten Daten genau zu planen. Sämtliche Anweisungen und Daten, alle Tupel und Indexe werden an die GPU übertragen, so dass diese selbstständig ihre Arbeit verrichten kann, ohne auf den Hauptspeicher oder die CPU zurückgreifen zu müssen. Bei Abschluss der Arbeit wird ausschließlich das Endprodukt der Verarbeitung per Bus an die CPU zurück transferiert. Dadurch lassen sich jedoch ausschließlich read-only Datenbankoperationen durchführen. Änderungen können durch den GPU Coprozessor nicht in akzeptabler Zeit realisiert werden.

Somit ist der Einsatz der GPU eher als Unterstützung zu sehen. Sie erlaubt es, der CPU Arbeit abzunehmen und CUDA kann somit parallel zu den anderen hier vorgestellten Algorithmen und Ansätzen zur Leistungssteigerung einer Datenbankanwendung eingesetzt werden.


\subsection{Mehrere Datenbank-Engines}
\label{sec:DBEngines}

Ein Ansatz der Optimierung von Datenbanken an ein Multicore-System ist es, bereits vorhandenes Wissen zu verwenden, dass bei der Arbeit mit verteilten Datenbanken gewonnen wurde. Hier gibt es schon Lösungen, wie man Effektiv Datenbankanfragen behandelt, wenn die Datenbank selber auf mehrere Server in einem Netzwerk verteilt sind. Die Idee ist, die Hardware als ein verteiltes System zu behandeln und auf die Versuche zu verzichten, die Parallelität durch verteilte Algorithmen zu verbergen. In diesem Sinne wird nicht die Behandlung einer Datenbankanfrage auf die Prozesse verteilt, sondern die Datenbank selber. Somit gibt es mehrere Datenbanksysteme, die auf dem selben Rechner laufen. Jeder Prozessor hat somit seine eigene Kopie der Datenbank. Das heißt, dass nicht etwa die Datenbankengine selber angepasst werden muss, um Multicore zu unterstützen, sondern mehrere Engines zum Einsatz kommen und nur noch eine Middleware-Schicht entwickelt werden muss, die die Anfragen auf die verteilte Datenbank-Engines vermittelt.

Multimed [Quelle: Database Engine on Multicore ] ist das Konzept einer solcherart aufgebauten Datenbank-Engine. Es setzt dabei auf eine Master-Datenbank und mehrere Satelliten-Datenbanken, die auf die Prozessoren verteilt sind. Der Vorteil hierbei ist es, dass ein solches System leicht skaliert werden kann, da sich mit steigenden Prozessorkern-Anzahl einfach die Anzahl der Satelliten-Datenbanken erhöht. Auch kann das gesamte Know-How der Verteilten Datenbanken verwendet werden. Alles was im LAN funktioniert, funktioniert auch in multimed. Nach außen hin tritt Miltimed jedoch wie eine einzelne Datenbankengien auf und kann wie eine solche verwendet und angesteuert werden. Updates werden auf der Master-Datenbank durchgeführt. Diese werden asynchron an die Satelliten weitergegeben. Datenbankanfragen die kein Update erfordern werden hingegen von den Satelliten durchgeführt. Um Konsistenz zu bewahren, werden Anfragen nur an Satelliten gesendet, die den aktuellen Stand, der dem Zeitpunkt der Anfrage entspicht, besitzen. Jedoch können hier auch beliebige andere Strategien mit all ihren Vor- und Nachteilen von verteilten Datenbanken zum Einsatz kommen.

Ein Problem von Multimed ist es jedoch wie in jeder verteilten Datenbank, dass Latenz Auftritt. Ein Datenbankbefehl wird im Falle eines Updates zum Master und bei normalen Anfragen an die Satelliten weitergeleitet. Durch diese Verwaltung und Weiterleitung entsteht eine geringe Latenzzeit, die nicht anfallen würde, wenn der Befehl direkt zum Datenbanksystem geleitet wird. Und da Updates alleine auf dem Master ausgeführt werden, ist das System bei Datenbankanforderungen mit vielen Änderungen an den Daten nicht ideal – der Master wird zum Bottleneck.

Zwar wird durch diesen Ansatz die Menge an Speicher pro Datenbank limitiert, jedoch sagen die Autoren des Papers [Quelle], das der Ansatz mehrere Datenbanken mit einem Teilspeicher schneller sei, als eine herkömmliche Datenbank, die über den gesamten Speicher verfügt.